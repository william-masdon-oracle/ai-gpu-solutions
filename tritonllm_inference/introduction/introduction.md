# Introduction

# Start the Workshop

## Task

In this workshop, we will focus on deploying large language models using Llama 3.2 with NVIDIA Triton Inference Server on Oracle Cloud Infrastructure (OCI). The workshop provides hands-on experience with implementing, deploying, and optimizing these models on OCI using either an A10 standalone shape or Oracle Kubernetes Engine (OKE). We will explore various stages of the deployment process, utilizing tools like TensorRT and Triton for scalable and efficient AI solutions.
Estimated Workshop Time: 1 hour

### Objectives

The objective of this workshop is to get hands-on experience with deploying and optimizing Llama 3.2 models using NVIDIA Triton Inference Server on OCI. You will learn how to efficiently implement AI solutions by leveraging GPU acceleration, and streamline deployment processes with automation tools such as Oracle Resource Manager and Terraform.

By the end of this workshop, you will:

* Understand how to deploy and optimize Llama 3.2 models using NVIDIA Triton Inference Server on OCI.
* Gain practical experience in utilizing TensorRT for inference optimization.
* Learn how to automate deployments with Oracle Resource Manager (ORM) or Terraform for one-click solutions.
* Scale AI deployments using either A10 standalone shapes or OKE on OCI.

### Prerequisites

This lab assumes you have:

* An Oracle Cloud account
* Administrator permissions or permissions to use the OCI Compute and Identity Domains
* Access to A10 or GPU shape, Usage of the Terraform code for one click deployment.

## Learn More

* [Use Llama 3 with NVIDIA TensorRT-LLM](https://docs.lxp.lu/howto/llama3-triton/)
* [Running Llama 3 with Triton and TensorRT-LLM](https://www.infracloud.io/blogs/running-llama-3-with-triton-tensorrt-llm/)

You may now proceed to the next lab.

## Acknowledgements

**Authors**

* **Bogdan Bazarca**, Senior Cloud Engineer, NACIE